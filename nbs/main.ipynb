{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Olyset vs Untreated Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from statistics import mode\n",
    "import openpyxl\n",
    "import tqdm\n",
    "\n",
    "import sys \n",
    "import joblib\n",
    "import pickle\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "sys.path.append('H:/Documents/PhD/itns/olyset-vs-untreated/src/')\n",
    "import extract, split, config\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Size and Overlap (in seconds)\n",
    "segment_size = 6\n",
    "segment_overlap = 5\n",
    "\n",
    "# Trials split between test/train and validation set\n",
    "test_trials = np.array([\n",
    "    0,1, 4,5,6, 9,10, 13,14, \n",
    "    17,18,19, 23,24,25, 29,30,31, 35,36,37])\n",
    "hyp_trials = np.array([\n",
    "    2,3, 7,8, 11,12, 15,16,  \n",
    "    20,21,22, 26,27,28, 32,33,34, 38,39])\n",
    "\n",
    "# Paths\n",
    "#results_path = config.PATH + 'tuned model/logistic-regression-mutual/' # Results stored\n",
    "results_path = config.PATH + 'tests/13-run/' # Results stored\n",
    "data_path = results_path + 'data/' # Any data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks, trackTargets, tracksTrialId, interpolated_flags = extract.load(config.FILE, config.PATH, config.IS_RESISTANT, config.DATA_PATH)\n",
    "\n",
    "interpolated = []\n",
    "for i in interpolated_flags:\n",
    "    interpolated += i\n",
    "\n",
    "with open(data_path + 'raw_tracks.npy', 'wb') as w:\n",
    "    np.save(w, np.array(tracks, dtype=object))\n",
    "with open(data_path + 'raw_trackTargets.npy', 'wb') as w:\n",
    "    np.save(w, np.array(trackTargets, dtype=object))\n",
    "with open(data_path + 'raw_tracksTrialId.npy', 'wb') as w:\n",
    "    np.save(w, np.array(tracksTrialId, dtype=object))\n",
    "with open(data_path + 'raw_interpolated.npy', 'wb') as w:\n",
    "    np.save(w, np.array(interpolated, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + 'raw_tracks.npy', 'wb') as w:\n",
    "    np.save(w, np.array(tracks, dtype=object))\n",
    "with open(data_path + 'raw_trackTargets.npy', 'wb') as w:\n",
    "    np.save(w, np.array(trackTargets, dtype=object))\n",
    "with open(data_path + 'raw_tracksTrialId.npy', 'wb') as w:\n",
    "    np.save(w, np.array(tracksTrialId, dtype=object))\n",
    "with open(data_path + 'raw_interpolated.npy', 'wb') as w:\n",
    "    np.save(w, np.array(interpolated, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.load(data_path + 'raw_tracks.npy', allow_pickle=True)\n",
    "trackTargets = np.load(data_path + 'raw_trackTargets.npy', allow_pickle=True)\n",
    "tracksTrialId = np.load(data_path + 'raw_tracksTrialId.npy', allow_pickle=True)   \n",
    "\n",
    "tracks = extract.generate_features(tracks, (0,1), 2)\n",
    "\n",
    "with open(data_path + 'tracks_features.npy', 'wb') as w:\n",
    "    np.save(w, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.load(data_path + 'tracks_features.npy', allow_pickle=True)\n",
    "\n",
    "track_id = 0\n",
    "while track_id < len(tracks):\n",
    "    track_interpolated = np.array(interpolated[track_id]).astype(bool)\n",
    "    tracks[track_id] = np.insert(tracks[track_id], len(tracks[track_id][0]), ~track_interpolated, axis=1)\n",
    "    track_id += 1\n",
    "\n",
    "with open(data_path + 'tracks_features_gaps_marked.npy', 'wb') as w:\n",
    "    np.save(w, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + 'tracks_features_gaps_marked.npy', 'wb') as w:\n",
    "    np.save(w, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.load(data_path + 'tracks_features_gaps_marked.npy', allow_pickle=True)\n",
    "trackTargets = np.load(data_path + 'raw_trackTargets.npy', allow_pickle=True)\n",
    "tracksTrialId = np.load(data_path + 'raw_tracksTrialId.npy', allow_pickle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for index, track in enumerate(tracks):\n",
    "    # Negative times\n",
    "    if np.any(track[:, 2] < 0):\n",
    "        indexes.append(index)\n",
    "    # Negative angular velocity\n",
    "    elif np.any(track[:, 4] < 0):\n",
    "        indexes.append(index)\n",
    "\n",
    "tracks = np.delete(tracks, indexes, axis=0)\n",
    "trackTargets = np.delete(trackTargets, indexes, axis=0)\n",
    "tracksTrialId = np.delete(tracksTrialId, indexes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks, trackTargets, tracksTrialId, trackGroup = split.split_tracks(\n",
    "    tracks, trackTargets, tracksTrialId, segment_size, segment_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lowest_frame_for_trial(tracks, trials):\n",
    "    unique_trials = np.unique(trials)\n",
    "    frames = dict()\n",
    "    for t in unique_trials:\n",
    "        select = np.where(trials == t)[0]\n",
    "        lowest = tracks[select[1]][0,16]\n",
    "\n",
    "        for track in tracks[select]:\n",
    "            try:\n",
    "                small = min(track[:,16])\n",
    "                if small < lowest:\n",
    "                    lowest = small\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        frames[f'{t}'] = lowest\n",
    "    return frames\n",
    "\n",
    "experiment_segment_time = []\n",
    "frames = find_lowest_frame_for_trial(tracks, tracksTrialId)\n",
    "\n",
    "for track_id, track in enumerate(tracks):\n",
    "    first_track_time = frames[str(tracksTrialId[track_id])]\n",
    "    earliest_time_in_exp = track[:, 16].min()\n",
    "    adj_earliest_time_in_exp = (earliest_time_in_exp-first_track_time)/50\n",
    "\n",
    "    experiment_segment_time.append(adj_earliest_time_in_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + 'tracks_split.npy', 'wb') as w:\n",
    "    np.save(w, tracks)\n",
    "with open(data_path + 'trackTargets_split.npy', 'wb') as w:\n",
    "    np.save(w, trackTargets)\n",
    "with open(data_path + 'trackGroup_split.npy', 'wb') as w:\n",
    "    np.save(w, trackGroup)\n",
    "with open(data_path + 'tracksTrialId_split.npy', 'wb') as w:\n",
    "    np.save(w, tracksTrialId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.load(data_path + 'tracks_split.npy', allow_pickle=True)\n",
    "trackTargets = np.load(data_path + 'trackTargets_split.npy', allow_pickle=True)\n",
    "trackGroup = np.load(data_path + 'trackGroup_split.npy', allow_pickle=True)  \n",
    "tracksTrialId = np.load(data_path + 'tracksTrialId_split.npy', allow_pickle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'X Velocity',\n",
    "    'Y Velocity',\n",
    "    'X Acceleration', \n",
    "    'Y Acceleration',\n",
    "    'Velocity',\n",
    "    'Acceleration',\n",
    "    'Jerk',\n",
    "    'Angular Velocity',\n",
    "    'Angular Acceleration',\n",
    "    'Angle of Flight',\n",
    "    'Centroid Distance Function',\n",
    "    'Persistence Velocity',\n",
    "    'Turning Velocity'\n",
    "]   \n",
    "indexes = [12,13,14,15,3,10,17,4,11,18,19,20,21]\n",
    "feature_stats = [\n",
    "    'mean','median','std', '1st quartile','3rd quartile','kurtosis', 'skewness',\n",
    "    'number of local minima','number of local maxima','number of zero-crossings']     \n",
    "\n",
    "track_statistics = dict()\n",
    "\n",
    "for col in feature_columns:\n",
    "    for stat in feature_stats:\n",
    "        track_statistics[f'{col} ({stat})'] = []\n",
    "\n",
    "for track in tracks:\n",
    "    data = extract.track_stats(track, indexes=indexes, columns=feature_columns)\n",
    "    for d in data:\n",
    "        track_statistics[d].append(data[d])\n",
    "\n",
    "df = pd.DataFrame(data=track_statistics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_add = extract.add_other_features(tracks, (0,1))\n",
    "df = pd.concat([df, to_add], axis=1)\n",
    "\n",
    "df = df.join(pd.DataFrame({'TrialID': tracksTrialId}))\n",
    "\n",
    "df_target = pd.DataFrame({\n",
    "    'Target': trackTargets, \n",
    "    'TrialID': tracksTrialId, \n",
    "    'TrackGroup': trackGroup,\n",
    "    'EarliestExpTime': experiment_segment_time\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(data_path + 'df_raw.pkl')\n",
    "df_target.to_pickle(data_path + 'df_target_raw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['EarliestExpTime'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(data_path + 'df_raw.pkl')\n",
    "df_target = pd.read_pickle(data_path + 'df_target_raw.pkl')\n",
    "tracks = np.load(data_path + 'tracks_split.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Penalty Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty_function(segment, n, m):\n",
    "    penalty_score = 0\n",
    "    k = 0\n",
    "\n",
    "    for position in segment:\n",
    "        if position == 0:\n",
    "            penalty_score += n * (m ** k)\n",
    "            k += 1\n",
    "        else:\n",
    "            k = max(0, k-1)\n",
    "\n",
    "    return penalty_score/len(segment)\n",
    "\n",
    "\n",
    "scores = []\n",
    "for segment in tracks:\n",
    "    mask = segment[:, -1]\n",
    "    scores.append(penalty_function(mask, n=1, m=1.05))\n",
    "scores = np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(scores, data_path + 'scores.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_threshold_list(start, end):\n",
    "    current_value = start\n",
    "    step_size = 1\n",
    "    score_thresholds = []\n",
    "    while current_value <= end:\n",
    "        score_thresholds.append(int(current_value))\n",
    "        if current_value < end/2:\n",
    "            current_value += step_size\n",
    "        else:\n",
    "            current_value += min(step_size, 500)\n",
    "        step_size = step_size * 1.1\n",
    "    score_thresholds = np.array(score_thresholds)\n",
    "    return score_thresholds\n",
    "\n",
    "\n",
    "def run_score_threshold_mutual_info(df, df_target, scores):\n",
    "    #score_thresholds = create_threshold_list(0, max(scores))\n",
    "    score_thresholds = np.linspace(0, max(scores), 250)\n",
    "    max_mutual_info_dict = {}\n",
    "    unique_features = []\n",
    "    for threshold in tqdm.tqdm(score_thresholds):\n",
    "        mask = np.where(scores <= threshold)[0]\n",
    "        df_temp = df.iloc[mask]\n",
    "        df_target_temp = df_target.iloc[mask]\n",
    "\n",
    "        indexes = df_temp[df_temp.isna().any(axis=1)].index\n",
    "        df_temp = df_temp.drop(index=indexes)\n",
    "        df_target_temp = df_target_temp.drop(index=indexes)\n",
    "\n",
    "        df_temp = extract.remove_nans(df_temp)        \n",
    "\n",
    "        df_temp = df_temp.drop(columns=['TrialID'])\n",
    "        df_target_temp = df_target_temp['Target']\n",
    "\n",
    "        mutual_info_values = mutual_info_classif(df_temp, df_target_temp)\n",
    "\n",
    "        unique_features += df_temp.columns.values.tolist()\n",
    "\n",
    "        for feature, mutual_info_value in zip(df_temp.columns, mutual_info_values):\n",
    "            if feature not in max_mutual_info_dict or max_mutual_info_dict[feature]['mutual_info'] < mutual_info_value:\n",
    "                max_mutual_info_dict[feature] = {\n",
    "                    'mutual_info': mutual_info_value,\n",
    "                    'score_threshold': threshold\n",
    "                }\n",
    "             \n",
    "            elif max_mutual_info_dict[feature]['mutual_info'] == mutual_info_value and max_mutual_info_dict[feature]['score_threshold'] < threshold:\n",
    "                max_mutual_info_dict[feature] = {\n",
    "                    'mutual_info': mutual_info_value,\n",
    "                    'score_threshold': threshold\n",
    "                }\n",
    "\n",
    "    unique_features = list(set(unique_features))\n",
    "    corresponding_values = [max_mutual_info_dict[feature]['score_threshold'] for feature in unique_features]\n",
    "\n",
    "    max_values = [max_mutual_info_dict[feature]['mutual_info'] for feature in unique_features]\n",
    "    exp = np.exp(np.array(max_values)/(np.array(corresponding_values)+1))\n",
    "    weights = exp / np.sum(exp)\n",
    "\n",
    "    weighted_average_threshold = np.average(corresponding_values, weights=weights)\n",
    "\n",
    "    return weighted_average_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = run_score_threshold_mutual_info(\n",
    "    df[df['TrialID'].isin(hyp_trials)],\n",
    "    df_target[df_target['TrialID'].isin(hyp_trials)], \n",
    "    scores[df_target['TrialID'].isin(hyp_trials)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "788444627122805.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.where(scores <= score_threshold)[0]\n",
    "df = df.iloc[mask]\n",
    "df_target = df_target.iloc[mask]\n",
    "\n",
    "indexes = df[df.isna().any(axis=1)].index\n",
    "df = df.drop(index=indexes)\n",
    "df_target = df_target.drop(index=indexes)\n",
    "\n",
    "df = extract.remove_nans(df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(data_path + 'df_filtered.pkl')\n",
    "df_target.to_pickle(data_path + 'df_target_filtered.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Train-Test/Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(data_path + 'df_filtered.pkl')\n",
    "df_target = pd.read_pickle(data_path + 'df_target_filtered.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['Banfora'] = 0\n",
    "df_target['Kisumu'] = 0\n",
    "df_target['Ngoussu'] = 0\n",
    "df_target['VK7'] = 0\n",
    "\n",
    "df_target.loc[df_target['TrialID'].isin([0,1,2,3, 17,18,19,20,21,22]), 'Banfora'] = 1\n",
    "df_target.loc[df_target['TrialID'].isin([4,5,6,7,8, 23,24,25,26,27,28]),'Kisumu'] = 1\n",
    "df_target.loc[df_target['TrialID'].isin([9,10,11,12, 29,30,31,32,33,34]),'Ngoussu'] = 1\n",
    "df_target.loc[df_target['TrialID'].isin([13,14,15,16, 35,36,37,38,39]),'VK7'] = 1\n",
    "\n",
    "df['Banfora'] = df_target['Banfora']\n",
    "df['Kisumu'] = df_target['Kisumu']\n",
    "df['Ngoussu'] = df_target['Ngoussu']\n",
    "df['VK7'] = df_target['VK7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['Resistant Status'] = 0\n",
    "\n",
    "df_target.loc[df_target['TrialID'].isin([0,1,2,3, 17,18,19,20,21,22]), 'Resistant Status'] = 1\n",
    "df_target.loc[df_target['TrialID'].isin([4,5,6,7,8, 23,24,25,26,27,28]),'Resistant Status'] = 0\n",
    "df_target.loc[df_target['TrialID'].isin([9,10,11,12, 29,30,31,32,33,34]),'Resistant Status'] = 0\n",
    "df_target.loc[df_target['TrialID'].isin([13,14,15,16, 35,36,37,38,39]),'Resistant Status'] = 1\n",
    "\n",
    "df['Resistant Status'] = df_target['Resistant Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Resistant Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['TrialID'].isin(test_trials)]\n",
    "df_train_target = df_target[df_target['TrialID'].isin(test_trials)]\n",
    "\n",
    "df_hyp = df[df['TrialID'].isin(hyp_trials)]\n",
    "df_hyp_target = df_target[df_target['TrialID'].isin(hyp_trials)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyp.to_pickle(data_path + 'df_hyp.pkl')\n",
    "df_hyp_target.to_pickle(data_path + 'df_hyp_target.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyp = df_hyp.drop(columns=['TrialID', 'Resistant Status'])\n",
    "\n",
    "sus = df_hyp[df_hyp_target['Target'] == 0]\n",
    "res = df_hyp[df_hyp_target['Target'] == 1]\n",
    "_, p_val = mannwhitneyu(sus, res)\n",
    "rej, p_vals_corrected, _, _ = multipletests(p_val, alpha=0.05, method='holm')\n",
    "\n",
    "columns = df_hyp.columns[rej]\n",
    "\n",
    "df_hyp = df_hyp.reset_index(drop=True)\n",
    "corr_matrix = df_hyp.corr(method='spearman').abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "\n",
    "cols = np.setdiff1d(columns, to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for feat in cols:\n",
    "    if ('TrialID' not in feat):\n",
    "        features.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for feat in cols:\n",
    "    if ('TrialID' not in feat):\n",
    "        features.append(feat)\n",
    "\n",
    "features += ['Resistant Status'] #['Kisumu', 'Banfora', 'Ngoussu', 'VK7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(results_path + 'features.txt', 'w+')\n",
    "for feat in features:\n",
    "    file.write(feat+'\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = open(results_path + 'features.txt', 'r+').read().split('\\n')\n",
    "features.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted(features):\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds():\n",
    "    final_folds = []\n",
    "    for v1 in list(itertools.product([0,1], [4,5,6], [9,10], [13,14])):\n",
    "        for v2 in list(itertools.product([17,18,19], [23,24,25], [29,30,31], [35,36,37])):\n",
    "            train_trials = list(v1) + list(v2)\n",
    "            final_folds.append(train_trials)\n",
    "    return final_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds():\n",
    "    final_folds = []\n",
    "    for v1 in itertools.product([0, 1], [4, 5, 6], [9, 10], [13, 14]):  # UT\n",
    "        for v2 in itertools.product(                                    # OL\n",
    "                itertools.combinations([17, 18, 19], 2), \n",
    "                itertools.combinations([23, 24, 25], 2), \n",
    "                itertools.combinations([29, 30, 31], 2), \n",
    "                itertools.combinations([35, 36, 37], 2)\n",
    "        ):\n",
    "            train_trials = list(v1) + [item for sublist in v2 for item in sublist]\n",
    "            final_folds.append(train_trials)\n",
    "    \n",
    "    return final_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = create_folds()\n",
    "folds = random.sample(folds, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(results_path+'folds.txt', 'w+')\n",
    "for f in folds:\n",
    "    file.write(str(f) +'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(results_path+'folds.txt', 'r+').read()\n",
    "file_split = file.split('\\n')\n",
    "file_split.remove('')\n",
    "folds = [eval(i) for i in file_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(folds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "\n",
    "def get_track_prediction(y_true, scores, preds, groups):\n",
    "    unique_groups = groups.unique()\n",
    "    track_preds = []\n",
    "    track_true = []\n",
    "    avg_scores = []\n",
    "    for val in unique_groups:\n",
    "        indexes = np.where(groups == val)[0]\n",
    "        track_true.append(mode(y_true.values[indexes]))\n",
    "        avg_scores.append(np.mean(scores[indexes]))\n",
    "        if np.mean(preds[indexes]) >= 0.5: \n",
    "            track_preds.append(1)\n",
    "        else:\n",
    "            track_preds.append(0)\n",
    "\n",
    "    return track_true, track_preds, avg_scores\n",
    "\n",
    "\n",
    "def produce_report(y_test, y_pred, scores):\n",
    "    precision_ir, recall_ir, _ = metrics.precision_recall_curve(y_test, scores, pos_label=1)\n",
    "    precision_is, recall_is, _ = metrics.precision_recall_curve(y_test, scores, pos_label=0)\n",
    "    \n",
    "    TP, FP, TN, FN = perf_measure(y_test, y_pred)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, scores)\n",
    "    data = {\n",
    "        'accuracy': metrics.accuracy_score(y_test,y_pred),\n",
    "        'balanced accuracy': metrics.balanced_accuracy_score(y_test,y_pred),\n",
    "        'roc auc': metrics.roc_auc_score(y_test, scores),\n",
    "\n",
    "        'f1 score (OL)': metrics.f1_score(y_test,y_pred, labels=np.unique(y_test), pos_label=1),\n",
    "        'precision (OL)': metrics.precision_score(y_test, y_pred, labels=np.unique(y_test), pos_label=1),\n",
    "        'recall (OL)': metrics.recall_score(y_test, y_pred, labels=np.unique(y_test), pos_label=1),\n",
    "        'pr auc (OL)': metrics.auc(recall_ir, precision_ir),\n",
    "\n",
    "        'f1 score (UT)': metrics.f1_score(y_test, y_pred, labels=np.unique(y_test), pos_label=0),\n",
    "        'precision (UT)': metrics.precision_score(y_test, y_pred, labels=np.unique(y_test), pos_label=0),\n",
    "        'recall (UT)': metrics.recall_score(y_test, y_pred, labels=np.unique(y_test), pos_label=0),\n",
    "        'pr auc (UT)': metrics.auc(recall_is, precision_is),\n",
    "\n",
    "        'cohen kappa': metrics.cohen_kappa_score(np.array(y_test), np.array(y_pred)),\n",
    "        'matthews correlation coefficient': metrics.matthews_corrcoef(np.array(y_test), np.array(y_pred)),\n",
    "        'log loss': metrics.log_loss(np.array(y_test), np.array(scores)),\n",
    "        'brier score': metrics.brier_score_loss(y_test, scores),\n",
    "        'tp': TP, 'tn': TN, 'fp': FP, 'fn': FN,\n",
    "        'tpr': tpr, 'fpr': fpr\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_model(x_train, y_train, x_test, y_test):\n",
    "    model = DummyRegressor(strategy='constant', constant=0)\n",
    "    model.fit(x_train, y_train['Target'])\n",
    "    y_pred = model.predict(x_test)\n",
    "    track_true, track_preds, scores = get_track_prediction(\n",
    "        y_test['Target'], y_pred, y_pred, y_test['TrackGroup'])\n",
    "    return produce_report(track_true, track_preds, scores), {\n",
    "        'segment-preds': y_pred, \n",
    "        'segment-target': y_test['Target'],\n",
    "        'segment-trial-ids': y_test['TrialID'],\n",
    "        'segment-track-group': y_test['TrackGroup'],\n",
    "        'segment-scores': y_pred\n",
    "    }  \n",
    "\n",
    "\n",
    "def xgboost_model(x_train, y_train, x_test, y_test, params, thresholds):\n",
    "    model = XGBClassifier(\n",
    "        **params\n",
    "    )\n",
    "    model.fit(x_train, y_train['Target'])\n",
    "    scores = model.predict_proba(x_test)[:,1]\n",
    "\n",
    "    if type(thresholds) == list:\n",
    "        results = []\n",
    "        for threshold in thresholds:\n",
    "            y_pred = (scores > threshold).astype(int)\n",
    "            track_true, track_preds, avg_scores = get_track_prediction(\n",
    "                y_test['Target'], scores, y_pred, y_test['TrackGroup'])\n",
    "            results.append((produce_report(track_true, track_preds, avg_scores), {\n",
    "                'segment-preds': y_pred, \n",
    "                'segment-target': y_test['Target'],\n",
    "                'segment-trial-ids': y_test['TrialID'],\n",
    "                'segment-track-group': y_test['TrackGroup'],\n",
    "                'segment-scores': scores\n",
    "            }, model))\n",
    "        return results\n",
    "    elif type(thresholds) == float:\n",
    "        y_pred = (scores > thresholds).astype(int)\n",
    "        track_true, track_preds, avg_scores = get_track_prediction(\n",
    "            y_test['Target'], scores, y_pred, y_test['TrackGroup'])\n",
    "        return (produce_report(track_true, track_preds, avg_scores), {\n",
    "            'segment-preds': y_pred, \n",
    "            'segment-target': y_test['Target'],\n",
    "            'segment-trial-ids': y_test['TrialID'],\n",
    "            'segment-track-group': y_test['TrackGroup'],\n",
    "            'segment-scores': scores\n",
    "        }, model)\n",
    "    raise ValueError('threshold wrong')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to undersample mask\n",
    "def undersample_mask(targets, mask, num_samples):\n",
    "    track_groups = targets[mask]['TrackGroup'].unique()\n",
    "    if len(track_groups) > num_samples:\n",
    "        sampled_track_groups = np.random.choice(track_groups, num_samples, replace=False)\n",
    "        track_groups_mask = targets['TrackGroup'].isin(sampled_track_groups)\n",
    "        return track_groups_mask & mask\n",
    "    else:\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((5, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict(\n",
    "    dummy_results_train = [],\n",
    "    dummy_results_test = [],\n",
    "    xgboost_train = [],\n",
    "    xgboost_test = [],\n",
    ")\n",
    "\n",
    "segment_scores_constant_train = []\n",
    "segment_scores_constant_test = []\n",
    "\n",
    "segment_scores_xgboost_train = []\n",
    "segment_scores_xgboost_test = []\n",
    "\n",
    "fold_threshold_results = []\n",
    "\n",
    "\n",
    "for index, fold in enumerate(folds):\n",
    "    print(f' --- FOLD {index} ---')\n",
    "    train_trials = fold\n",
    "    mask = df_train_target['TrialID'].isin(train_trials)\n",
    "\n",
    "    train = df_train[mask]\n",
    "    train_targets = df_train_target[mask]\n",
    "    \n",
    "    test = df_train[~mask]\n",
    "    test_targets = df_train_target[~mask]\n",
    "\n",
    "    # For OL train trials - before 30mins\n",
    "    # For UT train trials - randomly selected\n",
    "    ol_banfora_train_mask = train_targets['TrialID'].isin([17,18,19,20,21,22])\n",
    "    ol_kisumu_train_mask = train_targets['TrialID'].isin([23,24,25,26,27,28]) & (train_targets['EarliestExpTime'] <= 30*60)\n",
    "    ol_ngoussu_train_mask = train_targets['TrialID'].isin([29,30,31,32,33,34]) & (train_targets['EarliestExpTime'] <= 30*60)\n",
    "    ol_vk7_train_mask = train_targets['TrialID'].isin([35,36,37,38,39])\n",
    "\n",
    "    ut_banfora_train_mask = train_targets['TrialID'].isin([0,1,2,3])\n",
    "    ut_kisumu_train_mask = train_targets['TrialID'].isin([4,5,6,7,8,])\n",
    "    ut_ngoussu_train_mask = train_targets['TrialID'].isin([9,10,11,12])\n",
    "    ut_vk7_train_mask = train_targets['TrialID'].isin([13,14,15,16])\n",
    "\n",
    "    num_samples_kisumu =  len(train_targets[ol_kisumu_train_mask]['TrackGroup'].unique())\n",
    "    num_samples_ngoussu = len(train_targets[ol_ngoussu_train_mask]['TrackGroup'].unique())\n",
    "    num_samples = np.mean((num_samples_kisumu, num_samples_ngoussu)).astype(int)\n",
    "\n",
    "    # Undersample the masks\n",
    "    ol_banfora_train_mask_undersampled = undersample_mask(train_targets, ol_banfora_train_mask, num_samples)\n",
    "    ol_vk7_train_mask_undersampled = undersample_mask(train_targets, ol_vk7_train_mask, num_samples)\n",
    "\n",
    "    ut_banfora_train_mask_undersampled = undersample_mask(train_targets, ut_banfora_train_mask, num_samples)\n",
    "    ut_kisumu_train_mask_undersampled = undersample_mask(train_targets, ut_kisumu_train_mask, num_samples)\n",
    "    ut_ngoussu_train_mask_undersampled = undersample_mask(train_targets, ut_ngoussu_train_mask, num_samples)\n",
    "    ut_vk7_train_mask_undersampled = undersample_mask(train_targets, ut_vk7_train_mask, num_samples)\n",
    "\n",
    "    train_mask = ol_kisumu_train_mask | ol_ngoussu_train_mask | ol_banfora_train_mask_undersampled | ol_vk7_train_mask_undersampled | ut_banfora_train_mask_undersampled | ut_kisumu_train_mask_undersampled | ut_ngoussu_train_mask_undersampled | ut_vk7_train_mask_undersampled\n",
    "    train = train[train_mask]\n",
    "    train_targets = train_targets[train_mask]\n",
    "\n",
    "    # For OL test trials - before 30 mins\n",
    "    # For UT train trials - whole trial\n",
    "    ol_is_test_mask = test_targets['TrialID'].isin([23,24,25,26,27,28,29,30,31,32,33,34]) & (test_targets['EarliestExpTime'] <= 30*60)\n",
    "    ol_ir_test_mask = test_targets['TrialID'].isin([17,18,19,20,21,22,35,36,37,38,39])\n",
    "    ut_is_test_mask = test_targets['TrialID'].isin([4,5,6,7,8,9,10,11,12])\n",
    "    ut_ir_test_mask = test_targets['TrialID'].isin([0,1,2,3,13,14,15,16])\n",
    "\n",
    "    test_mask = ol_is_test_mask | ol_ir_test_mask | ut_is_test_mask | ut_ir_test_mask\n",
    "    test = test[test_mask]\n",
    "    test_targets = test_targets[test_mask]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train = pd.DataFrame(scaler.fit_transform(train[features]), columns=features, index=train.index)\n",
    "    test = pd.DataFrame(scaler.transform(test[features]), columns=features, index=test.index)\n",
    "\n",
    "    #sm = SMOTE(\n",
    "    #    random_state=0\n",
    "    #)\n",
    "    #train_os, train_targets_os = sm.fit_resample(train, train_targets.drop(columns=['TrialID','TrackGroup']))\n",
    "    train_os = train\n",
    "    train_targets_os = train_targets.drop(columns=['TrialID', 'TrackGroup'])\n",
    "\n",
    "    constant_scores, constant_segment_scores = constant_model(\n",
    "        x_train=train_os, \n",
    "        y_train=train_targets_os, \n",
    "        x_test=train, \n",
    "        y_test=train_targets)\n",
    "    results['dummy_results_train'].append(constant_scores)\n",
    "    segment_scores_constant_train.append(constant_segment_scores)\n",
    "\n",
    "    constant_scores, constant_segment_scores = constant_model(\n",
    "        x_train=train_os, \n",
    "        y_train=train_targets_os, \n",
    "        x_test=test, \n",
    "        y_test=test_targets)\n",
    "\n",
    "    results['dummy_results_test'].append(constant_scores)\n",
    "    segment_scores_constant_test.append(constant_segment_scores)\n",
    "    \n",
    "\n",
    "    negative_examples = train_targets_os[train_targets_os['Target'] == 0]\n",
    "    positive_examples = train_targets_os[train_targets_os['Target'] == 1]\n",
    "    scale_pos_weight = len(negative_examples) / len(positive_examples)\n",
    "\n",
    "\n",
    "    thresholds = np.arange(0.01, 1, 0.01).tolist()\n",
    "    threshold_results = xgboost_model(\n",
    "        x_train=train_os, \n",
    "        y_train=train_targets_os, \n",
    "        x_test=train, \n",
    "        y_test=train_targets,\n",
    "        params=dict(\n",
    "            random_state=0,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=200,\n",
    "            max_depth=7,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.9,\n",
    "            reg_alpha=0.01, \n",
    "            reg_lambda=0.1,\n",
    "            min_child_weight=5,\n",
    "            scale_pos_weight=scale_pos_weight   \n",
    "        ),\n",
    "        thresholds=thresholds)\n",
    "\n",
    "    best_index = np.argmax([i[0]['matthews correlation coefficient'] for i in threshold_results])\n",
    "    results['xgboost_train'].append(threshold_results[best_index][0])\n",
    "    segment_scores_xgboost_train.append(threshold_results[best_index][1])\n",
    "    best_threshold = thresholds[best_index]\n",
    "    print(best_threshold)\n",
    "\n",
    "    fold_threshold_results.append(threshold_results)\n",
    "\n",
    "    xg_scores, segment_scores, model = xgboost_model(\n",
    "        x_train=train_os, \n",
    "        y_train=train_targets_os, \n",
    "        x_test=test, \n",
    "        y_test=test_targets,\n",
    "        params=dict(\n",
    "            random_state=0,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=200,\n",
    "            max_depth=7,\n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.9,\n",
    "            reg_alpha=0.01, \n",
    "            reg_lambda=0.1,\n",
    "            min_child_weight=5,\n",
    "            scale_pos_weight=scale_pos_weight \n",
    "        ),\n",
    "        thresholds=best_threshold)\n",
    "    results['xgboost_test'].append(xg_scores)\n",
    "    segment_scores_xgboost_test.append(segment_scores)\n",
    "    \n",
    "    joblib.dump(dict(\n",
    "        model=model,\n",
    "        df_train=df_train,\n",
    "        df_train_target=df_train_target,\n",
    "        features=features,\n",
    "        test=test,\n",
    "        test_targets=test_targets,\n",
    "        mask=mask,\n",
    "        train_os=train_os,\n",
    "    ), data_path+f'shap/xgboost_shap_dump_{index}.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def produce_report(y_test, y_pred, scores):\n",
    "    precision_ir, recall_ir, _ = metrics.precision_recall_curve(y_test, scores, pos_label=1)\n",
    "    precision_is, recall_is, _ = metrics.precision_recall_curve(y_test, scores, pos_label=0)\n",
    "    \n",
    "    TP, FP, TN, FN = perf_measure(y_test, y_pred)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, scores)\n",
    "    data = {\n",
    "        'accuracy': metrics.accuracy_score(y_test,y_pred),\n",
    "        'balanced accuracy': metrics.balanced_accuracy_score(y_test,y_pred),\n",
    "        'roc auc': metrics.roc_auc_score(y_test, scores),\n",
    "\n",
    "        'f1 score (OL)': metrics.f1_score(y_test,y_pred, labels=np.unique(y_test), pos_label=1),\n",
    "        'precision (OL)': metrics.precision_score(y_test, y_pred, labels=np.unique(y_test), pos_label=1),\n",
    "        'recall (OL)': metrics.recall_score(y_test, y_pred, labels=np.unique(y_test), pos_label=1),\n",
    "        'pr auc (OL)': metrics.auc(recall_ir, precision_ir),\n",
    "\n",
    "        'f1 score (UT)': metrics.f1_score(y_test, y_pred, labels=np.unique(y_test), pos_label=0),\n",
    "        'precision (UT)': metrics.precision_score(y_test, y_pred, labels=np.unique(y_test), pos_label=0),\n",
    "        'recall (UT)': metrics.recall_score(y_test, y_pred, labels=np.unique(y_test), pos_label=0),\n",
    "        'pr auc (UT)': metrics.auc(recall_is, precision_is),\n",
    "\n",
    "        'cohen kappa': metrics.cohen_kappa_score(np.array(y_test), np.array(y_pred)),\n",
    "        'matthews correlation coefficient': metrics.matthews_corrcoef(np.array(y_test), np.array(y_pred)),\n",
    "        'log loss': metrics.log_loss(np.array(y_test), np.array(scores)),\n",
    "        'brier score': metrics.brier_score_loss(y_test, scores),\n",
    "        'tp': TP, 'tn': TN, 'fp': FP, 'fn': FN,\n",
    "        'tpr': tpr, 'fpr': fpr\n",
    "    }\n",
    "    return data\n",
    "\n",
    "strain_results = dict(\n",
    "    banfora_train = [],\n",
    "    banfora_test = [],\n",
    "    kisumu_train = [],\n",
    "    kisumu_test = [],\n",
    "    ngoussu_train = [],\n",
    "    ngoussu_test = [],\n",
    "    vk7_train = [],\n",
    "    vk7_test = []\n",
    ")\n",
    "for data in segment_scores_xgboost_test:\n",
    "    # Banfora\n",
    "    mask = data['segment-trial-ids'].isin([0,1,2,3, 17,18,19,20,21,22])\n",
    "    track_true, track_preds, avg_scores = get_track_prediction(\n",
    "            data['segment-target'][mask], \n",
    "            data['segment-preds'][mask], \n",
    "            data['segment-scores'][mask], \n",
    "            data['segment-track-group'][mask])\n",
    "    strain_results['banfora_test'].append(produce_report(track_true, track_preds, avg_scores))\n",
    "\n",
    "    # Kisumu\n",
    "    mask = data['segment-trial-ids'].isin([4,5,6,7,8, 23,24,25,26,27,28])\n",
    "    track_true, track_preds, avg_scores = get_track_prediction(\n",
    "            data['segment-target'][mask], \n",
    "            data['segment-preds'][mask], \n",
    "            data['segment-scores'][mask], \n",
    "            data['segment-track-group'][mask])\n",
    "    strain_results['kisumu_test'].append(produce_report(track_true, track_preds, avg_scores))\n",
    "\n",
    "    # Ngoussu\n",
    "    mask = data['segment-trial-ids'].isin([9,10,11,12, 29,30,31,32,33,34])\n",
    "    track_true, track_preds, avg_scores = get_track_prediction(\n",
    "            data['segment-target'][mask], \n",
    "            data['segment-preds'][mask], \n",
    "            data['segment-scores'][mask], \n",
    "            data['segment-track-group'][mask])\n",
    "    strain_results['ngoussu_test'].append(produce_report(track_true, track_preds, avg_scores))\n",
    "\n",
    "    # VK7\n",
    "    mask = data['segment-trial-ids'].isin([13,14,15,16, 35,36,37,38,39])\n",
    "    track_true, track_preds, avg_scores = get_track_prediction(\n",
    "            data['segment-target'][mask], \n",
    "            data['segment-preds'][mask], \n",
    "            data['segment-scores'][mask], \n",
    "            data['segment-track-group'][mask])\n",
    "    strain_results['vk7_test'].append(produce_report(track_true, track_preds, avg_scores))\n",
    "\n",
    "for data in segment_scores_xgboost_train:\n",
    "    # Banfora\n",
    "    mask = data['segment-trial-ids'].isin([0,1,2,3, 17,18,19,20,21,22])\n",
    "    track_true, track_preds, avg_scores = get_track_prediction(\n",
    "            data['segment-target'][mask], \n",
    "            data['segment-preds'][mask], \n",
    "            data['segment-scores'][mask], \n",
    "            data['segment-track-group'][mask])\n",
    "    strain_results['banfora_train'].append(produce_report(track_true, track_preds, avg_scores))\n",
    "\n",
    "    # Kisumu\n",
    "    mask = data['segment-trial-ids'].isin([4,5,6,7,8, 23,24,25,26,27,28])\n",
    "    track_true, track_preds, avg_scores = get_track_prediction(\n",
    "            data['segment-target'][mask], \n",
    "            data['segment-preds'][mask], \n",
    "            data['segment-scores'][mask], \n",
    "            data['segment-track-group'][mask])\n",
    "    strain_results['kisumu_train'].append(produce_report(track_true, track_preds, avg_scores))\n",
    "\n",
    "    # Ngoussu\n",
    "    mask = data['segment-trial-ids'].isin([9,10,11,12, 29,30,31,32,33,34])\n",
    "    track_true, track_preds, avg_scores = get_track_prediction(\n",
    "            data['segment-target'][mask], \n",
    "            data['segment-preds'][mask], \n",
    "            data['segment-scores'][mask], \n",
    "            data['segment-track-group'][mask])\n",
    "    strain_results['ngoussu_train'].append(produce_report(track_true, track_preds, avg_scores))\n",
    "\n",
    "    # VK7\n",
    "    mask = data['segment-trial-ids'].isin([13,14,15,16, 35,36,37,38,39])\n",
    "    track_true, track_preds, avg_scores = get_track_prediction(\n",
    "            data['segment-target'][mask], \n",
    "            data['segment-preds'][mask], \n",
    "            data['segment-scores'][mask], \n",
    "            data['segment-track-group'][mask])\n",
    "    strain_results['vk7_train'].append(produce_report(track_true, track_preds, avg_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = [\n",
    "    'balanced accuracy', 'f1 score (OL)',\n",
    "    'f1 score (UT)',  'cohen kappa', 'matthews correlation coefficient',\n",
    "    'log loss', 'brier score']\n",
    "\n",
    "for m in ms:\n",
    "    plt.figure()\n",
    "    for res in fold_threshold_results:\n",
    "        plt.plot(np.arange(0.01, 1, 0.01), [i[0][m] for i in res], alpha=0.8)\n",
    "    plt.xlabel('Decision score threshold')\n",
    "    plt.ylabel(m)\n",
    "    plt.title(f'Change in {m} for varying decision score threshold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_path+'results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "with open(results_path+'scores.pkl', 'wb') as f:\n",
    "    pickle.dump(segment_scores_xgboost_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(results_path+'folds.txt', 'w+')\n",
    "for fold in folds:\n",
    "    file.write(str(fold) +'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_path+'results.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.create_sheet()\n",
    "\n",
    "row = 2\n",
    "metrics_list = list(results['xgboost_train'][0].keys())\n",
    "for i, column in enumerate(['model'] + metrics_list):\n",
    "    sheet.cell(row=1, column=i+1).value = column\n",
    "\n",
    "for model in ['dummy_results', 'xgboost']:\n",
    "    for model_type in ['train', 'test']:\n",
    "        try:\n",
    "            sheet.cell(row=row, column=1).value = f'{model.upper()} {model_type.upper()}'\n",
    "            for j, metric in enumerate(results[model+'_'+model_type][0].keys()):\n",
    "                if metric in ['tp', 'tn', 'fp', 'fn']:\n",
    "                    scores = 0\n",
    "                    for fold in range(len(results[model+'_'+model_type])):\n",
    "                        scores += results[model+'_'+model_type][fold][metric]\n",
    "                    \n",
    "                    sheet.cell(row=row, column=j+2).value = scores \n",
    "                elif metric in ['tpr', 'fpr']:\n",
    "                    pass\n",
    "                else:\n",
    "                    scores = []\n",
    "                    for fold in range(len(results[model+'_'+model_type])):\n",
    "                        scores.append(results[model+'_'+model_type][fold][metric])\n",
    "                    \n",
    "                    sheet.cell(row=row, column=j+2).value = f'{round(np.mean(scores), 3)} ({round(min(scores), 3)} - {round(max(scores), 3)})'\n",
    "            row += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "wb.save(results_path + 'scores.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.create_sheet()\n",
    "\n",
    "row = 2\n",
    "metrics_list = list(strain_results['banfora_train'][0].keys())\n",
    "for i, column in enumerate(['model'] + metrics_list):\n",
    "    sheet.cell(row=1, column=i+1).value = column\n",
    "\n",
    "for model in ['banfora', 'kisumu', 'ngoussu', 'vk7']:\n",
    "    for model_type in ['train', 'test']:\n",
    "        try:\n",
    "            sheet.cell(row=row, column=1).value = f'{model.upper()} {model_type.upper()}'\n",
    "            for j, metric in enumerate(strain_results[model+'_'+model_type][0].keys()):\n",
    "                if metric in ['tp', 'tn', 'fp', 'fn']:\n",
    "                    scores = 0\n",
    "                    for fold in range(len(strain_results[model+'_'+model_type])):\n",
    "                        scores += strain_results[model+'_'+model_type][fold][metric]\n",
    "                    \n",
    "                    sheet.cell(row=row, column=j+2).value = scores \n",
    "                elif metric in ['tpr', 'fpr']:\n",
    "                    pass\n",
    "                else:\n",
    "                    scores = []\n",
    "                    for fold in range(len(strain_results[model+'_'+model_type])):\n",
    "                        scores.append(strain_results[model+'_'+model_type][fold][metric])\n",
    "                    \n",
    "                    sheet.cell(row=row, column=j+2).value = f'{round(np.mean(scores), 3)} ({round(min(scores), 3)} - {round(max(scores), 3)})'\n",
    "            row += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "wb.save(results_path + 'scores-strain.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CONFUSION MATRIX'''\n",
    "\n",
    "def plot_confusion_matrix(tp, tn, fp, fn, classifier):\n",
    "    confusion_matrix = np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "\n",
    "    im = ax.imshow(confusion_matrix, cmap='Oranges')\n",
    "\n",
    "    ax.set_xlabel('Predicted Class')\n",
    "    ax.set_ylabel('True Class')\n",
    "\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(['IS', 'IR'])\n",
    "    ax.set_yticklabels(['IS', 'IR'])\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, confusion_matrix[i, j], ha='center', va='center', fontsize=14)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix_percent(tp, tn, fp, fn, classifier):\n",
    "    confusion_matrix = np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "\n",
    "    im = ax.imshow(confusion_matrix, cmap='Oranges')\n",
    "\n",
    "    ax.set_xlabel('Predicted Class')\n",
    "    ax.set_ylabel('True Class')\n",
    "\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(['UT', 'OL'])\n",
    "    ax.set_yticklabels(['UT', 'OL'])\n",
    "\n",
    "    total_samples = np.sum(confusion_matrix)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            percentage = confusion_matrix[i, j] / total_samples * 100\n",
    "            ax.text(j, i, f'{percentage:.2f}%', ha='center', va='center', color='black', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for key in ['dummy_results_train', 'dummy_results_test','xgboost_train', 'xgboost_test']:\n",
    "    print(key)\n",
    "    plot_confusion_matrix_percent(\n",
    "        sum([_['tp'] for _ in results[key]]), \n",
    "        sum([_['tn'] for _ in results[key]]), \n",
    "        sum([_['fp'] for _ in results[key]]), \n",
    "        sum([_['fn'] for _ in results[key]]),\n",
    "        classifier=key\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ROC CURVES'''\n",
    "\n",
    "def plot_roc_curve(tpr, fpr, classifier):\n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "\n",
    "    for i in range(len(tpr)):\n",
    "        ax.plot(fpr[i], tpr[i], alpha=0.5)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for key in ['dummy_results_train', 'dummy_results_test','xgboost_train', 'xgboost_test']:\n",
    "    print(key)\n",
    "    plot_roc_curve(\n",
    "        [_['tpr'] for _ in results[key]],  \n",
    "        [_['fpr'] for _ in results[key]], \n",
    "        classifier=key\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PR CURVES'''\n",
    "\n",
    "def plot_pr_curve(precision, recall, classifier):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    base = np.linspace(0, 1, 101)\n",
    "    avg_precision = 1\n",
    "\n",
    "    for i in range(len(precision)):\n",
    "        ax.plot(recall[i], precision[i])\n",
    "        new_avg_precision = sum(precision[i]) / len(precision[i])\n",
    "        if new_avg_precision < avg_precision:\n",
    "            avg_precision = new_avg_precision\n",
    "\n",
    "    ax.plot(base, [avg_precision]*len(base), linestyle='--')\n",
    "\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title(f'PR Curves - {classifier}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for key in ['dummy_results_train', 'dummy_results_test','xgboost_train','xgboost_test']:\n",
    "    plot_pr_curve(\n",
    "        [results[key][i]['precision (ir) (curve)'] for i in range(len(results[key]))],\n",
    "        [results[key][i]['recall (ir) (curve)'] for i in range(len(results[key]))], \n",
    "        classifier=key+' (OL)'\n",
    "    )\n",
    "    plot_pr_curve(\n",
    "        [results[key][i]['precision (is) (curve)'] for i in range(len(results[key]))],\n",
    "        [results[key][i]['recall (is) (curve)'] for i in range(len(results[key]))], \n",
    "        classifier=key+' (UT)'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''EXCEL FILE OF ALL FOLD SCORES'''\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "for key in ['dummy_results_train', 'dummy_results_test','xgboost_train','xgboost_test']:\n",
    "    sheet = wb.create_sheet(key.upper())\n",
    "    columns = ['fold', 'test trials', 'train trials', 'accuracy', 'balanced accuracy',\n",
    "    'f1 score (OL)', 'roc auc', 'precision (OL)', 'recall (OL)', 'pr auc (OL)',\n",
    "    'f1 score (UT)', 'precision (UT)', 'recall (UT)', 'pr auc (UT)',\n",
    "    'cohen kappa', 'matthews correlation coefficient', 'log loss', 'tp', 'tn', 'fp', 'fn']\n",
    "    for i, column in enumerate(columns):\n",
    "        sheet.cell(row=1, column=i+1).value = column\n",
    "\n",
    "        for row in range(len(results[key])):\n",
    "            if column not in ['fold', 'test trials', 'train trials']:\n",
    "                sheet.cell(row=row+2, column=i+1).value = results[key][row][column]\n",
    "        \n",
    "            elif column == 'fold':\n",
    "                sheet.cell(row=row+2, column=i+1).value = row\n",
    "\n",
    "            elif column == 'test trials':\n",
    "                train = folds[row]\n",
    "                all_ids = df_train_target['TrialID'].unique()\n",
    "                sheet.cell(row=row+2, column=i+1).value = str([x for x in all_ids if x not in train])\n",
    "\n",
    "            elif column == 'train trials':\n",
    "                sheet.cell(row=row+2, column=i+1).value = str(folds[row])\n",
    "\n",
    "wb.save(results_path + \"all-folds.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94d2f0a72470cbfa5bf040eb69ac68719f2f2aa4e7158c9baf86afd24ebc134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
